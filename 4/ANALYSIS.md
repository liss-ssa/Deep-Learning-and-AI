# Анализ результатов

## 1. Сравнение CNN и полносвязных сетей

### 1.1 Результаты на MNIST

**Таблица результатов:**
| Модель            | Точность | Время обучения | Параметры |
|-------------------|----------|----------------|-----------|
| FC Network        | 97.98%   | 143s           | 535K      |
| Simple CNN        | 99.33%   | 401s           | 421K      |
| Residual CNN      | 99.28%   | 1086s          | 160K      |

**График:**
`plots/MNIST/cm_SimpleCNN.png` - Confusion matrix для CNN

**Анализ:**
- CNN превосходят FC на 1.35% при меньшем числе параметров
- Residual блоки не дали преимущества на MNIST (избыточность)
- Оптимальный выбор - SimpleCNN (баланс точности и времени)

### 1.2 Результаты на CIFAR-10

**Таблица результатов:**
| Модель               | Точность | Время обучения | Параметры |
|----------------------|----------|----------------|-----------|
| FC Network           | 53.26%   | 222s           | 1.7M      |
| Residual CNN         | 79.72%   | 1031s          | 161K      |
| Regularized ResCNN   | 76.80%   | 362s           | 620K      |

**График:**
`plots/CIFAR-10/cm_ResidualCNN.png` - Confusion matrix

**Анализ:**
- CNN превосходят FC на 26.46%
- Residual связи критически важны для CIFAR-10
- Регуляризация ускоряет обучение в 3 раза, но снижает точность

## 2. Анализ архитектур CNN

### 2.1 Влияние размера ядра свертки

**Таблица результатов:**
| Конфигурация | Точность | Рецептивное поле |
|--------------|----------|------------------|
| 3×3          | 72.20%   | 7                |
| 5×5          | 74.56%   | 9                | 
| 7×7          | 65.18%   | 7                |
| 1×1+3×3      | 66.59%   | 5                |

**Ключевые графики:**
1. `plots/kernel_size_results.png` - Сравнение точности и RF
2. `plots/activations_kernel_3x3.png` - Активации для 3×3
3. `plots/activations_kernel_5x5.png` - Активации для 5×5

**Анализ:**
- Оптимальный размер - 5×5 (баланс RF и параметров)
- 7×7 ядра слишком велики для CIFAR-10
- Комбинация 1×1+3×3 ухудшает результаты

### 2.2 Влияние глубины сети

**Таблица результатов:**
| Архитектура       | Точность | Градиенты (μ±σ)       |
|-------------------|----------|-----------------------|
| 2 слоя            | 70.88%   | 2.80e-02 ± 2.62e-02   |
| 4 слоя            | 82.66%   | 1.00e-03 ± 1.39e-03   |
| 6 слоев           | 82.38%   | 4.82e-03 ± 8.01e-03   |
| ResNet (6 слоев)  | 81.11%   | 2.38e-02 ± 2.36e-02   |

**Ключевые графики:**
1. `plots/depth_results.png` - Сравнение глубинных архитектур
2. `plots/features_depth_Medium.png` - Feature maps (4 слоя)
3. `plots/features_depth_ResNet.png` - Feature maps ResNet

**Анализ:**
- Оптимум на 4 слоях (82.66%)
- ResNet демонстрирует более стабильные градиенты
- Глубокие сети (>4 слоев) требуют Residual связей

## 3. Исследование кастомных слоев

**Таблица результатов:**
| Слой         | Точность | Время обучения |
|--------------|----------|----------------|
| Стандартный  | 64.59%   | 215s           |
| CustomConv   | 63.68%   | 263s           |
| Attention    | 64.93%   | 273s           |
| Swish        | 63.17%   | 215s           |

**Ключевые графики:**
1. `plots/custom_layers_comparison.png` - Сравнение метрик
2. `plots/standard_features.png` - Feature maps стандартной
3. `plots/attention_features.png` - Feature maps с attention

**Анализ:**
- Attention дает минимальный прирост (+0.34%)
- Кастомные слои не оправдывают затрат
- Swish ухудшает результаты на 1.42%

## Общие выводы

1. **Для простых данных (MNIST):**
   - Достаточно 2-3 Conv слоев с ядрами 3×3
   - Residual блоки избыточны

2. **Для сложных данных (CIFAR-10):**
   - Оптимальная архитектура:
     - 4-5 Conv слоев
     - Ядра 5×5 в первых слоях
     - Residual связи при глубине >4 слоев

3. **Кастомные слои:**
   - Требуют тщательной настройки
   - Дают marginal gain на небольших моделях

4. **Критические факторы:**
   ```mermaid
   graph TD
   A[Эффективность CNN] --> B[Размер ядра]
   A --> C[Глубина сети] 
   C --> D[Residual связи]
   A --> E[Регуляризация]
   B --> F[Рецептивное поле]