# Анализ результатов

## Задание 1

### Общие наблюдения

- MNIST: Все модели достигли высокой точности (>92%), что ожидаемо для простого датасета.
- CIFAR-10: Точность значительно ниже (35-55%), что подтверждает сложность задачи для полносвязных сетей.
- Глубина сети: Увеличение глубины улучшает результаты, но после 3-5 слоёв прирост минимален.

### Анализ MNIST

| Слои          | Параметры | Train Acc | Test Acc | Разрыв (переобучение)   |
|---------------|----------:|----------:|---------:|------------------------:|
| 1 (линейный)  | 7,850     | 92.76%    | 92.38%   | 0.38% (минимальное)     |
| 2             | 203,530   | 99.47%    | 97.68%   | 1.79%                   |
| 3             | 235,146   | 99.32%    | 97.89%   | 1.43%                   |
| 5             | 575,050   | 99.31%    | 98.04%   | 1.27%                   |
| 7             | 903,498   | 99.09%    | 97.89%   | 1.20%                   |

`Выводы:`

- Линейный классификатор (1 слой) показывает хорошие результаты (92%)
- Добавление слоёв увеличивает точность, но после 3 слоёв прирост незначителен
- Переобучение контролируемое (разрыв Train/Test < 2%)

### Анализ CIFAR-10

| Слои | Параметры | Train Acc | Test Acc | Разрыв |
|------|----------:|----------:|---------:|-------:|
| 1    | 30,730    | 38.14%    | 35.13%   | 3.01%  |
| 2    | 789,258   | 62.39%    | 49.84%   | 12.55% |
| 3    | 820,874   | 65.66%    | 52.10%   | 13.56% |
| 5    | 1,746,506 | 68.52%    | 53.88%   | 14.64% |
| 7    | 2,074,954 | 68.71%    | 53.71%   | 15.00% |

`Выводы:`

- Сильное переобучение (разрыв до 15%), особенно для глубоких сетей
- Лучшая модель: 5 слоёв (Test Acc = 53.88%)
- Дальнейшее увеличение глубины не даёт преимуществ

### Влияние регуляризации (CIFAR-10, 5 слоёв)

| Метод               | Train Acc | Test Acc | Разрыв  | Эффективность               |
|---------------------|----------:|---------:|--------:|----------------------------:|
| Без регуляризации   | 68.52%    | 53.88%   | 14.64%  | Сильное переобучение        |
| Dropout             | 61.92%    | 53.85%   | 8.07%   | Лучший баланс               |
| BatchNorm           | 41.55%    | 40.10%   | 1.45%   | Сильно ограничивает модель  |
| Оба метода          | 40.98%    | 40.86%   | 0.12%   | Избыточная регуляризация    |

`Ключевые выводы:`

- Dropout (p=0.5): Уменьшает переобучение в 2 раза без сильной потери точности
- BatchNorm: Слишком агрессивно ограничивает модель (падение accuracy на ~13%)
- Комбинация (Dropout + BatchNorm): Не даёт преимуществ перед Dropout

## Задание 2

###  Сравнение моделей разной ширины

| Конфигурация       | Параметры | Train Acc | Test Acc | Разрыв (Train-Test) | Время обучения (сек) |
|--------------------|----------:|----------:|---------:|--------------------:|---------------------:|
| narrow [64,32,16]  | 199,450   | 58.3%     | 49.7%    | 8.6%               | ~100                |
| medium [256,128,64]| 828,490   | 66.7%     | 53.1%    | 13.6%              | ~150                |
| wide [1024,512,256]| 3,805,450 | 67.9%     | 53.7%    | 14.2%              | ~250                |
| xwide [2048,1024,512]| 8,921,610 | 66.7%     | 53.1%    | 13.6%              | ~350                |

`Выводы:`

Точность:

- Увеличение ширины слоёв с 64 до 1024 нейронов даёт прирост Test Accuracy с 49.7% до 53.7%
- Дальнейшее увеличение (xwide) не улучшает результаты — наблюдается насыщение
- Оптимальная ширина: 1024-512-256 (Test Acc = 53.7%)

Переобучение:

- Разрыв между Train/Test Accuracy растёт с увеличением ширины (с 8.6% до 14.2%), что указывает на усиление переобучения.

Производительность:

- Время обучения увеличивается пропорционально количеству параметров
- wide vs xwide: +100 сек при незначительном приросте точности

### Оптимизация архитектуры

| Схема                     | Test Acc | Параметры  | Особенности               |
|---------------------------|---------:|-----------:|--------------------------:|
| narrowing [1024,512,256]  | 54.1%    | 3,805,450  | Классическое сужение      |
| expanding [256,512,1024]  | 53.4%    | 1,453,834  | Обратная схема            |
| constant [512,512,512]    | 52.7%    | 2,103,818  | Равномерное распределение |
| bottleneck [1024,256,1024]| 53.7%    | 3,682,570  | "Бутылочное горлышко"     |

`Ключевые наблюдения:`

- учшая схема: narrowing (54.1%) — сужение сети показывает стабильно высокую точность
- Bottleneck: Не уступает narrowing (53.7%), но требует больше параметров
- Expanding: Наихудший результат (53.4%) — расширяющиеся сети хуже обобщают
- Constant: Средний результат (52.7%) — компромисс между сложностью и точностью

Для CIFAR-10 оптимальна схема сужения (narrowing) с ширинами [1024, 512, 256].

### Визуализация результатов

`Accuracy vs Ширина слоёв:`

- График показывает рост Test Accuracy до wide-конфигурации с последующим плато
- narrow: Сильное ограничение модели (низкая точность)
- wide/xwide: Схожие результаты, но wide быстрее

`Heatmap схем изменения ширины:`

- Чётко видно преимущество narrowing перед другими подходами
- Bottleneck и constant близки по эффективности

## Задание 3

### Основные наблюдения:

Без регуляризации (no_reg):

- Сильное переобучение: разрыв train/test accuracy 0.7546 vs 0.5280
- Test accuracy не меняется после 5-й эпохи, тогда как train accuracy продолжает расти
- Типичный случай overfitting - модель "запоминает" тренировочные данные

Dropout:

- Dropout 0.1: уменьшил переобучение (разрыв 0.6421 vs 0.5423), но accuracy ниже базового
- Dropout 0.3: лучший баланс (разрыв 0.5161 vs 0.5177)
- Dropout 0.5: слишком сильная регуляризация - accuracy падает (0.4031 vs 0.4577)

BatchNorm:

- Лучший результат среди базовых методов: test accuracy 0.5585
- Ускоряет сходимость (быстрый рост accuracy в первых эпохах)
- Но все еще заметное переобучение (0.7991 vs 0.5585)

Комбинации:

- Dropout 0.3 + BatchNorm показал лучший баланс (0.5994 vs 0.5720)
- Меньший разрыв между train/test по сравнению с чистым BatchNorm
Б- олее стабильное обучение (меньшие колебания test accuracy)

L2 регуляризация:

- Эффект похож на Dropout 0.1, но результаты хуже
- Test accuracy не превышает 0.45

Адаптивные методы:

- Прогрессивный Dropout: хорошие результаты (0.5940 vs 0.5197)
- BatchNorm с переменным momentum: лучший test accuracy (0.5518), но сильное переобучение
- Слоистая комбинация: самый стабильный результат (0.5418 vs 0.5271)

### Сравнение методов регуляризации

| Метод                     | Train Acc | Test Acc | Разрыв  | Особенности                     |
|---------------------------|----------:|---------:|--------:|--------------------------------:|
| Без регуляризации         | 75.46%    | 52.80%   | 22.66%  | Сильное переобучение           |
| Dropout 0.1               | 64.21%    | 54.23%   | 9.98%   | Умеренная эффективность        |
| Dropout 0.3               | 51.61%    | 51.77%   | -0.16%  | Лучший баланс                  |
| BatchNorm                 | 79.91%    | 55.85%   | 24.06%  | Ускорение сходимости           |
| Dropout 0.3 + BatchNorm   | 59.94%    | 57.20%   | 2.74%   | Оптимальная комбинация         |
| L2 регуляризация          | 44.79%    | 44.39%   | 0.40%   | Низкая эффективность           |

### Адаптивные методы

| Метод                          | Test Acc | Стабильность (std) |
|--------------------------------|---------:|-------------------:|
| Прогрессивный Dropout          | 51.97%   | 0.008             |
| BatchNorm с переменным momentum| 55.18%   | 0.012             |
| Слоистая комбинация            | 52.71%   | 0.005             |
